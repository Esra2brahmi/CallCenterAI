# .github/workflows/lint-test.yml
name: "Reusable: Lint & Test (Services)"

on:
  workflow_call:
    inputs:
      service-path:
        description: "Path to the service package (e.g. src/services/tfidf_service)"
        required: true
        type: string
      service-name:
        description: "Short name used for coverage flags and artifacts"
        required: true
        type: string

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - mode: fast
            marker: "not integration"
          - mode: integration
            marker: "integration"

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-v1-${{ hashFiles('requirements.txt') }}
          restore-keys: pip-v1-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov httpx sentence-transformers "numpy<2.0.0"

      - name: Add project root to PYTHONPATH
        run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV

      # LA SEULE CHOSE QU'ON AJOUTE – 100 % magique, 0 modification ailleurs
      - name: Setup dummy MLflow registry + fake models (100% working – 2025 proof)
        run: |
          set -euo pipefail

          # 1. Crée le répertoire + experiment 0
          mkdir -p mlruns/0
          cat > mlruns/0/meta.yaml <<EOF
          experiment_id: 0
          name: Default
          lifecycle_stage: active
          artifact_location: file://${{ github.workspace }}/mlruns/0
          EOF

          # 2. Crée un modèle dummy pyfunc valide + enregistre les deux modèles nécessaires
          python - <<'PY'
          import mlflow
          import mlflow.pyfunc
          import numpy as np
          import os

          mlflow.set_tracking_uri(f"file://{os.getenv('GITHUB_WORKSPACE')}/mlruns")

          # Classe valide pour mlflow.pyfunc
          class DummyModel(mlflow.pyfunc.PythonModel):
              def predict(self, context, model_input):
                  if isinstance(model_input, str):
                      return ["mock"]
                  elif hasattr(model_input, '__len__') and len(model_input) > 0 and isinstance(model_input[0], str):
                      return ["mock"] * len(model_input)
                  else:
                      return ["mock"]

          with mlflow.start_run(experiment_id="0") as run:
              run_id = run.info.run_id

              # Log du modèle dummy (nom correct pour éviter le warning)
              mlflow.pyfunc.log_model(
                  artifact_path="model",
                  python_model=DummyModel(),
                  registered_model_name="dummy_for_ci_only"  # on log sous un nom temporaire
              )

              # Enregistrement direct des deux modèles que ton code cherche
              client = mlflow.MlflowClient()
              model_names = ["tfidf_svm_ticket_classifier", "CallCenterTransformer"]

              for name in model_names:
                  try:
                      client.create_registered_model(name)
                  except:
                      pass  # déjà existe

                  # Enregistre depuis le run
                  mv = client.create_model_version(
                      name=name,
                      source=f"{run.info.artifact_uri}/model",
                      run_id=run_id
                  )
                  client.transition_model_version_stage(
                      name=name,
                      version=mv.version,
                      stage="Production",
                      archive_existing_versions=True
                  )
                  print(f"Dummy model registered: {name} v{mv.version} → Production")
          PY

          echo "MLFLOW_TRACKING_URI=file://${{ github.workspace }}/mlruns" >> $GITHUB_ENV

      - name: Run tests
        run: |
          pytest tests \
            -m "${{ matrix.marker }}" \
            --cov="${{ inputs.service-path }}" \
            --cov-report=xml:coverage.xml \
            --junitxml=junit-${{ inputs.service-name }}-${{ matrix.mode }}.xml \
            -vv

      - name: Upload coverage to Codecov
        if: matrix.mode == 'fast'
        uses: codecov/codecov-action@v4
        with:
          files: coverage.xml
          flags: ${{ inputs.service-name }}

      - name: Upload JUnit report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: junit-${{ inputs.service-name }}-${{ matrix.mode }}
          path: junit-*.xml