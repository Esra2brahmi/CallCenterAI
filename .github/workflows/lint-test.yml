# .github/workflows/lint-test.yml
name: "Reusable: Lint & Test (Services)"

on:
  workflow_call:
    inputs:
      service-path:
        description: "Path to the service package (e.g. src/services/tfidf_service)"
        required: true
        type: string
      service-name:
        description: "Short name used for coverage flags and artifacts"
        required: true
        type: string

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        include:
          - mode: fast
            marker: "not integration"
          - mode: integration
            marker: "integration"

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-v1-${{ hashFiles('requirements.txt') }}
          restore-keys: pip-v1-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov httpx sentence-transformers "numpy<2.0.0"

      - name: Add project root to PYTHONPATH
        run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV

      # LA SEULE CHOSE QU'ON AJOUTE – 100 % magique, 0 modification ailleurs
      - name: Create dummy MLflow model registry so imports don't crash
        run: |
          mkdir -p mlruns
          cat > mlruns/meta.yaml <<'EOF'
          artifact_uri: file:///home/runner/work/CallCenterAI/CallCenterAI/mlruns
          experiment_id: "0"
          lifecycle_stage: active
          name: Default
          EOF
          
          # Dummy registered models – couvre les 3 services existants
          python -c "
          import mlflow
          from pathlib import Path
          mlflow.set_tracking_uri('file://${{ github.workspace }}/mlruns')
          
          class Dummy:
              def predict(self, x): return ['mock'] if isinstance(x, str) else ['mock'] * len(x)
              def predict_proba(self, x): import numpy as np; return np.array([[0.1, 0.9]])
              classes_ = ['mock_class']
          
          with mlflow.start_run():
              mlflow.pyfunc.log_model('model', python_model=Dummy(), artifact_path='model')
              run_id = mlflow.active_run().info.run_id
          
          # On enregistre les 3 modèles que ton code cherche
          for name in ['tfidf_svm_ticket_classifier', 'CallCenterTransformer', 'agent_router']:
              uri = f'runs:/{run_id}/model'
              try:
                  mlflow.register_model(uri, name)
                  print(f'Registered dummy model: {name}')
              except:
                  pass
          "

      - name: Run tests
        run: |
          pytest tests \
            -m "${{ matrix.marker }}" \
            --cov="${{ inputs.service-path }}" \
            --cov-report=xml:coverage.xml \
            --junitxml=junit-${{ inputs.service-name }}-${{ matrix.mode }}.xml \
            -vv

      - name: Upload coverage to Codecov
        if: matrix.mode == 'fast'
        uses: codecov/codecov-action@v4
        with:
          files: coverage.xml
          flags: ${{ inputs.service-name }}

      - name: Upload JUnit report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: junit-${{ inputs.service-name }}-${{ matrix.mode }}
          path: junit-*.xml