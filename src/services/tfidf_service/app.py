# file: app/main.py
import logging
import os
import re
import string
from typing import Dict

import mlflow
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from prometheus_fastapi_instrumentator import Instrumentator
from nltk.corpus import stopwords

# ==========================================================
# Configuration via variables d'environnement
# ==========================================================
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
MODEL_NAME = os.getenv("MLFLOW_MODEL_NAME", "tfidf_svm_ticket_classifier")  # nom du mod√®le enregistr√©
MODEL_STAGE = os.getenv("MLFLOW_MODEL_STAGE", "Production")                # ou "Staging", "None"

mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

app = FastAPI(
    title="Ticket Classification API ‚Äì TF-IDF + Calibrated LinearSVC",
    version="1.2.0",
    description="Pr√©dit le Topic_group d‚Äôun ticket √† partir de son texte",
)

Instrumentator().instrument(app).expose(app)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.info(f"MLflow tracking URI : {MLFLOW_TRACKING_URI}")
logger.info(f"Chargement du mod√®le {MODEL_NAME} en stade {MODEL_STAGE}")


# ==========================================================
# Chargement du mod√®le (avec retry + fallback tr√®s clair)
# ==========================================================
"""def load_model_from_mlflow():
    model_uri = f"models:/{MODEL_NAME}/{MODEL_STAGE}"
    logger.info(f"Tentative de chargement depuis MLflow ‚Üí {model_uri}")

    try:
        model = mlflow.sklearn.load_model(model_uri)
        logger.info(f"Mod√®le charg√© avec succ√®s depuis MLflow ({MODEL_NAME} v{model._model_meta.run_id[:8]}...)")
        return model
    except mlflow.exceptions.MlflowException as e:
        logger.error(f"MLflow ne trouve pas le mod√®le {MODEL_NAME} en {MODEL_STAGE} : {e}")

        # Essai avec la derni√®re version "None" (cas fr√©quent apr√®s le 1er enregistrement)
        try:
            client = mlflow.MlflowClient()
            latest = client.get_latest_versions(MODEL_NAME, stages=["None"])[0]
            model_uri = f"models:/{MODEL_NAME}/{latest.version}"
            logger.info(f"Utilisation de la version {latest.version} (stage=None)")
            model = mlflow.sklearn.load_model(model_uri)
            logger.info("Mod√®le charg√© depuis la derni√®re version disponible")
            return model
        except Exception:
            pass

    raise RuntimeError(f"Impossible de charger le mod√®le {MODEL_NAME} depuis MLflow")"""
# ==========================================================
# Chargement du mod√®le ‚Äì version bulletproof 2025
# ==========================================================
def load_model_from_mlflow():
    logger.info(f"Recherche du mod√®le {MODEL_NAME} (stage pr√©f√©r√©: {MODEL_STAGE})")

    client = mlflow.MlflowClient()

    # 1. Essai avec le stage demand√© (Production, Staging, etc.)
    try:
        latest = client.get_latest_versions(MODEL_NAME, stages=[MODEL_STAGE])
        if latest:
            version = latest[0].version
            model_uri = f"models:/{MODEL_NAME}/{version}"
            logger.info(f"Chargement version {version} (stage {MODEL_STAGE})")
            return mlflow.sklearn.load_model(model_uri)
    except Exception as e:
        logger.warning(f"Pas de version en {MODEL_STAGE} ‚Üí {e}")

    # 2. Fallback : derni√®re version quel que soit le stage
    try:
        latest_any = client.get_latest_versions(MODEL_NAME, stages=None)  # None = toutes
        if latest_any:
            version = latest_any[0].version
            model_uri = f"models:/{MODEL_NAME}/{version}"
            logger.info(f"Fallback ‚Üí version {version} (stage {latest_any[0].current_stage})")
            return mlflow.sklearn.load_model(model_uri)
    except Exception as e:
        logger.error(f"Aucune version trouv√©e pour {MODEL_NAME} ‚Üí {e}")

    raise RuntimeError(f"Mod√®le {MODEL_NAME} introuvable dans le Model Registry")


# Chargement au d√©marrage
try:
    model = load_model_from_mlflow()
except Exception as e:
    logger.critical(f"√âchec critique du chargement du mod√®le : {e}")
    raise e


# ==========================================================
# Nettoyage du texte (identique √† ton pipeline d‚Äôentra√Ænement)
# ==========================================================
try:
    STOP_WORDS = set(stopwords.words("english"))
except LookupError:
    import nltk
    nltk.download('stopwords')
    STOP_WORDS = set(stopwords.words("english"))


def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    text = re.sub(r"\d+", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    words = [w for w in text.split() if w not in STOP_WORDS]
    return " ".join(words)


# ==========================================================
# Schemas
# ==========================================================
class PredictionRequest(BaseModel):
    text: str = Field(..., description="Texte brut du ticket", example="My printer is not working anymore")


class PredictionResponse(BaseModel):
    predicted_label: str
    probabilities: Dict[str, float]
    cleaned_text: str
    model_version: str = None


# ==========================================================
# Routes
# ==========================================================
@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    if not request.text.strip():
        raise HTTPException(status_code=400, detail="Le champ 'text' ne peut pas √™tre vide")

    cleaned = clean_text(request.text)

    try:
        probas = model.predict_proba([cleaned])[0]
        pred_idx = probas.argmax()
        predicted_label = model.classes_[pred_idx]

        prob_dict = {str(cls): float(p) for cls, p in zip(model.classes_, probas)}

        return PredictionResponse(
            predicted_label=str(predicted_label),
            probabilities=prob_dict,
            cleaned_text=cleaned,
            model_version=model._model_meta.run_id[:8] if hasattr(model, "_model_meta") else "unknown"
        )
    except Exception as e:
        logger.exception("Erreur pendant la pr√©diction")
        raise HTTPException(status_code=500, detail="Erreur interne du mod√®le")


@app.get("/health")
def health():
    return {
        "status": "healthy",
        "model": MODEL_NAME,
        "stage": MODEL_STAGE,
        "tracking_uri": MLFLOW_TRACKING_URI,
        "classes": list(model.classes_)
    }


@app.get("/")
def root():
    return {"message": "Ticket Classification API ‚Äì TF-IDF + SVM", "docs": "/docs"}

"""import logging
import os
import re
import string
import socket
from fastapi import FastAPI, HTTPException
import joblib
from pydantic import BaseModel
import mlflow
from nltk.corpus import stopwords
from prometheus_fastapi_instrumentator import Instrumentator



MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)

print(f"üöÄ MLflow URI automatically selected: {MLFLOW_TRACKING_URI}")


# ==========================================================
# ‚öôÔ∏è Config
# ==========================================================
MODEL_NAME = "tfidf_svm_model"
MODEL_STAGE = "Production"
LOCAL_MODEL_PATH = "models/tfidf_svm_model.pkl"

app = FastAPI(title="TF-IDF SVM Service", version="1.1.0")
Instrumentator().instrument(app).expose(app)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==========================================================
# üßπ Text cleaning
# ==========================================================
def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""

    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    text = re.sub(r"\d+", "", text)

    stop_words = set(stopwords.words("english"))
    text = " ".join(w for w in text.split() if w not in stop_words)

    return text


# ==========================================================
# üì¶ Load MLflow model with fallback
# ==========================================================
def load_model():
    try:
        model_uri = f"models:/{MODEL_NAME}/{MODEL_STAGE}"
        logger.info(f"Attempting to load MLflow model: {model_uri}")

        model = mlflow.sklearn.load_model(model_uri)
        logger.info(f"‚úÖ MLflow model loaded successfully: {MODEL_NAME} ({MODEL_STAGE})")
        return model

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Could not load MLflow model: {e}")

        if os.path.exists(LOCAL_MODEL_PATH):
            logger.info(f"üì¶ Loading fallback local model: {LOCAL_MODEL_PATH}")
            return joblib.load(LOCAL_MODEL_PATH)

        raise RuntimeError("‚ùå No valid ML model available (MLflow failed, no local model).")


model = load_model()


# ==========================================================
# üßæ API Schemas
# ==========================================================
class PredictionRequest(BaseModel):
    text: str


class PredictionResponse(BaseModel):
    label: str
    probability: dict
    cleaned: str


# ==========================================================
# üîÆ Prediction Endpoint
# ==========================================================
@app.post("/predict", response_model=PredictionResponse)
def predict(req: PredictionRequest):

    if not req.text.strip():
        raise HTTPException(400, "Text is required")

    try:
        cleaned = clean_text(req.text)
        probs = model.predict_proba([cleaned])[0]
        label = model.classes_[probs.argmax()]
        probs_dict = {cls: float(p) for cls, p in zip(model.classes_, probs)}

        return PredictionResponse(
            label=label,
            probability=probs_dict,
            cleaned=cleaned
        )

    except Exception as e:
        logger.exception("Prediction failed")
        raise HTTPException(500, f"Prediction error: {e}")


# ==========================================================
# ‚ù§Ô∏è Health Check
# ==========================================================
@app.get("/health")
def health():
    return {
        "status": "healthy",
        "loaded_model": MODEL_NAME,
        "stage": MODEL_STAGE,
        "tracking_uri": MLFLOW_TRACKING_URI
    }
"""